# Razones 
me estoy yendo por los cerros de √∫beda. En un momneto dado estoy revisando (o a√±adiendo)bibliograf√≠a, que viendo como funciona stui, pasando por un c√≥mo ser√≠a reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementac√≠on en python,por esta raz√≥n voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto
1. Siguiendo los pasos en el fichero instalaci√≥nSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desarrollar de forma m√°s cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la m√°quina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a trav√©s de ssh.
3. Intento de instalaci√≥n del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalaci√≥n de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios d√≠as)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librer√≠a ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(¬øde forma binaria o discreta?)-> mirar uno de los papers de Delia y Jorge, se menciona el ventilador .
- Buscar una forma m√°s fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparaci√≥n btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se est√° haciendo la medici√≥n baja la utilizaci√≥n de la cpu al no atenderse el promt(¬øal ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reuni√≥n mencion√≥ algo de la configuraci√≥n throtlling- ¬øQu√© era exactamente? -> mirar las notas.
- ¬øTiene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases m√©tricas. averiguar que otras m√©tricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser √∫til es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- A√±adir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de medici√≥n). El comportamiento lo detallar√© cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustar√≠a hacer un script de bash que sea capaz de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo junto con algunas anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts est√°n casi terminadas (faltan los m√©todos update). Es√≥ s√≠, la metricas de prompts  y la alimentaci√≥n solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentac√≠on de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es m√°s dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan f√°cil como otros lenguajes de programaci√≥n).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo m√°s f√°cil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una medici√≥n-> parece que Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podr√≠a afectar a la medici√≥n[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer m√©tricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metricas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issue anterior pueden ser √∫tiles-> mirar y decidir luego.
    -  Ver si llama.cpp tambien las devuelve y si no como obtenerlas.-> si las devuelve.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todav√≠a no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests m√°s limpios en paralelo.
- Seg√∫n el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, tengo demasiadas ventanas abiertas en el navegador üôÉ)

# 2025-04-27: 
## Instalaci√≥n y verificaci√≥n del SO BASE
Hoy en esta tarde tan agradable de domingo voy a proceder a instalra el so en a otra tarjeta microsd (un sandisk extreme pro64 gb,sdxc 1, v30 A2 luego actualizar√© con ma≈õdetalles ) a fin de tener un sistema operativo limpio donde realizar las mediciones: 
```bash
sudo apt update && sudo apt full-upgrade #actualiza el software a la √∫ltima versi√≥n 
sudo rpi-eeprom-update # comprobamos el firmware
# (Mon 23 Sep 13:02:56 UTC 2024 (1727096576)
# no actualizo anque ahay una versi√≥n disponble (spoiler actualizar√© luego)
```
Procedo a modicar el bootloader, activo el autologin(por defecto al ser la version lite de raspbian(la misma que se indica en so base), no tiene entorno de escritorio).
```bash 
sudo raspi-config
```
Procedo tambien a aumentar la swap a 1024 MB para por si acaso (previamente 512 ya que los modelos a ejecutar tienen versiones de parametros que rondan los 7b por lo que pueden necesitar mas ram de la disponible y por tanto hacer uso de la swap): 
```bash 
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile #CONF_SWAPSIZE=1024 
sudo dphys-swapfile setup 
sudo dphys-swapfile swapon # luego de instalar todo lo reiniciar√©
```
Procedo a comprobar el DVFS, aunque en la RPI5 el dvfs se controla con el gobernador, no sin antes mirar la documetaci√≥n del [configtxt](https://www.raspberrypi.com/documentation/computers/config_txt.html) para ver que informaci√≥n puede ser relevante luego -> no toco nada y lo dejo tal como est√° (tomo nota de que arm boost est√° a 1).
Compruebo el modo del gobernor(mientras buscaba info que referenciar para el gobernador, he encontrado esta pagina [web](https://bret.dk/raspberry-pi-5-review/) con varios benchmarks de la rpi, no tiene nada que ver pero quizas pueda ser √∫til en un futuro); el gobernador est√° puesto a ondemand, lo dejo as√≠.
''' bash 
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor 
'''
Procedo a actualizar la eeprom .
```bash
sudo rpi-eeprom -update -a
```
Compruebo la versi√≥n de python instalada-> python 3.11.2  y reinicio 

## Instalaci√≥n del software de hailo()
Solo me voy a dedicar a hacer la instalaci√≥n software(siguiendo la misma [gu√≠a](https://github.com/TFG-yisuscc/hailo-rpi5-examples/blob/main/doc/install-raspberry-pi5.md#how-to-set-up-raspberry-pi-5-and-hailo) que la vez anterior):

1.  habilito pcie3(recordatorio tambi√©n de porqu√© est√° [deshabilitado por defecto](https://blog.adafruit.com/2023/10/13/forcing-pci-express-gen-3-0-speeds-on-the-pi-5-piday-raspberrypi-geerlingguy-raspberry_pi/))
2. desenchufo la rpi e instalo el AI KIT 
3. instalo las dependencias (parece que descarga paquetes de iconos y todo, el adwaita).
4. Reinicio y compruebo 
5. no lo detecta compruebo el cable que une a la rpi con el hat.
6. Funciona.
7. no desmonto el ai kit. 

Durante todo este proceso, he observado que los tiempos de instalaci√≥n han disminuido bastante, al menos comparado con lo que recuerdo de la otra vez. -> muy problablemente sea por la nueva micro sd(no creo que sea por el firmware pero tambi√©n podr√≠a deberse a eso). Es posible que tengas que abordarlo cuando elabores la memoria y muy probablemente tengas  que establecer una comparativa.

### Vuelvo a la tarjeta micro sd anterior(la verbatim):
1. Instalo los driver hailo,
2. Me aseguro de que el firmwre de la eeprom est√© actualizado
Verifico que a la hora de instalar los drivers la tarjeta verbatim es  m√°s lenta.
Hago una micro  prueba de rendimiento. -> con el hailo instalado (y por ende sin el ventilador del la carcasa) rondaba los 100 grados seg√∫n btop y empez√≥ a a ralentizarse -> detuve la prueba por si acaso .

# 2025-05-01
(Los cambios los estoy estoy haciendolos en una (rama aparte[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_mejora_clase_prompt_metrics]))
La idea general sobre qu√© m√©tricas considerar y c√≥mo medirlas est√° m√°s o menos definida (aunque con Ollama). 
El concepto es el siguiente: desarrollar un programa que, dado un conjunto de modelos, una frecuencia de muestreo y una lista de prompts, obtenga m√©tricas relacionadas con cada prompt y con el hardware. Esto se realizar√° de la siguiente forma , mientras se procesa cada prompt en paralelo, A intervalos regulares, se tomar√°n medidas del hardware. Este proceso se repetir√° para cada modelo.

Falta lo siguiente(sin orden particular): 

- Mediciones relativas al sensor de temperatura de hailo [Enlace relevante](https://community.hailo.ai/t/how-to-get-temperature-of-hailo8l-device/1943).

- Pulir el c√≥digo.

- Incorporar las funciones relacionadas con llama.cpp.

-Corregir peque√±os errores en las clases Hardware metrics relacionados con los PIDS-> realmente no creo que sea un error 

- Separar las variables/parametros configurables por el usuario del programa(frecuencia,modelos espaciado temporal minimo entre prompts  o entre modelos...).

- Mirar si usar logs en lugar de csv.
 - Lo descrito en las issues del repositorio

Pero antes de  ponerme con eso, voy a hablar con Delia y Jorge antes de continuar, para asegurarme de que voy bien encaminado(que √∫ltimamente estoy despistad√≠simo).
PD: Este [repo](https://github.com/aidatatools/ollama-benchmark.git) es interesante
# 2025-05-03

Viendo y comparando los tiempos en los prompt, me preocupa que la forma de medir actual est√© interfiriendo(no me extra√±ar√≠a en absoluto), ya que en comparaci√≥n con la otra forma(la de dos hilos paralelos sin sincronizaci√≥n) tarda mucho en llegar al 100% de cpu, mientras que cuando lo monitorizaba aparte con btop en cuanto se cargaba usaba el 100 de la cpu.-> correcci√≥n puede deberse a que simplemente est√° cargando el modelo.

Con los ex√°menes  la vuelta de la esquina, voy a tener que dejarlo un poco aparcado, asi que me voy a establecer un roadmap :
1. Convertir las carpetas a paquetes(error de novato) y configurar correctamente el gitignore-> hecho, aunque probablemente tenga que mejorarlo luego
2. Completar llama.cpp.
3. Configurar el keepalive de ollama para que se desactive con el √∫ltmo prompt(en el caso de llama que se descarge de la memoria). 
2. Pasar de csv puro a log o mantenerlo 
3. Crear el configuration.py

# 2025-05-04: 
Bueno me dedico a continuar  las funciones relacionadas con llama.cpp, parece que aunque por defecto en los modelos LLama  devuelvan  las metrics de perf, estas no son tan f√°cilmente "accesibles" ni est√°n en la misma magnitud(milisegundos vs nanosegundos) como la de ollama, considero lo siguiente: 

1. Trastear con la forma en la que llama da las respuestas: 
    - Mediante cambios en el formato de chat que usa o la api que usa.
    - Mediante RegEx-> lo he intentado durante unas horas y lo voy a descartar, muy problem√°tico y no queda limpio(para mi gusto)
2. Calcularlas manualmente, esto tiene el siguiente problema(entre otros): 
La marca de tiempo de fin - la de inicio no es coincidente con la duraci√≥n total expresada por ollama, por ejemplo: 
```
prompt_id,start_timestamp,finish_timestamp,model,total_duration,prompt_eval_count,prompt_eval_duration,eval_count,load_duration
0,1746372420759336778,1746372530928341446,llama3.2:latest,110165606442,100,10625576984,343,34353355410
1,1746372531432656953,1746372598822507332,llama3.2:latest,67387207186,63,4713146930,326,34342030
```
 En la primera fila: 
 1746372530928341446-1746372420759336778 = 110169004668 != 110165606442 .
 Supone una diferencia de tiempo de aproximadaamente 3,39 milisegundos.

Mientras que en la segunda fila:
1746372598822507332-1746372531432656953 = 67389850379 != 67387207186
Lo que supone una diferencia de 2,64 milisegundos aproximadamente.
Aunque es poco, sin saber cuanta precisi√≥n voy a necesitar, prefiero explorar esta opci√≥n mas tarde si fuera necesario.

3. Viendo el (codigo fuente)[https://github.com/abetlen/llama-cpp-python/blob/99f2ebfde18912adeb7f714b49c1ddb624df3087/llama_cpp/llama_cpp.py#L4223] puedo crear una clase auxiliar que haga un parseo de los datos(quiz√°s  incluso una clase que la extienda)ya que la libreria de llama_cpp_python no lo ofrece  -> es lo que estoy haciendo ahora en la rama (dev_llama_cpp_function)[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_llama_cpp_functions] en utils.llama_utils, aunque tengo la duda de que llama al darte los datos en ms en lugar de ns (como hace ollama), ¬øa que tenemos que convertir?, ollama a ms o llama a ns , personalmente me decanto por lo primero, pero voy a esperar a ma√±ana.

# 2025-05-08:
Rama llama.cpp functions
Me he dado cuenta de que faltaba el eval_duration en la clase promp_metrics-> corregido.


Seg√∫n el codigo fuente de [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/51fb96b1ff2e1cc98b2492a012b7d93531a6a9a8/src/llama-context.cpp#L2428) el total duration se calcula restando el tiempo de t_start- t_end. Sin  embargo, como se puede observar, ese t_end no nos lo porporciona el objeto data,  sino que se obtine del propio sistema- > voy a aprovechar las timestamps.
# 2025-05-16
Antes de que se me olvide, este [entrada](https://learn.arm.com/learning-paths/embedded-and-microcontrollers/llama-python-cpu/llama-python-chatbot/) de la pagina de arm es interesante, sobretodo para las estimaciones de memoria ram

He estado estos dias limpiando el repositorio(mergeando ramas y eliminando ramas en remoto que ya no se usan, etc) y como bien dije en la presentaci√≥n las funciones  de ollama  est√°n completas.

Respecto a las funciones de llama, est√°n casi listas y un poco m√°s organizados(en una rama llamada llama_cpp_class_n_function, probalemente no la suba al remoto)
Falta por determinar como se realizar√°n las mediciones de llama(es decir falta el equivalente de main_ollama pero para llama) y como se recibiran los modelos de llama(en ollama simplemente lo especificamos mediante una cadena de texto), supongo que lo har√© mediante rutas (absolutas) al archivo.

Me pongo a estudiar rob√≥tica y espero no morir en el intento 