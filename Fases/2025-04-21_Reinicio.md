# Razones 
me estoy yendo por los cerros de Ãºbeda. En un momneto dado estoy revisando (o aÃ±adiendo)bibliografÃ­a, que viendo como funciona stui, pasando por un cÃ³mo serÃ­a reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementacÃ­on en python,por esta razÃ³n voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto  
1. Siguiendo los pasos en el fichero instalaciÃ³nSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desarrollar de forma mÃ¡s cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la mÃ¡quina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a travÃ©s de ssh.
3. Intento de instalaciÃ³n del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalaciÃ³n de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios dÃ­as)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librerÃ­a ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(Â¿de forma binaria o discreta?)-> mirar uno de los papers de Delia y Jorge, se menciona el ventilador .
- Buscar una forma mÃ¡s fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparaciÃ³n btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se estÃ¡ haciendo la mediciÃ³n baja la utilizaciÃ³n de la cpu al no atenderse el promt(Â¿al ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reuniÃ³n mencionÃ³ algo de la configuraciÃ³n throtlling- Â¿QuÃ© era exactamente? -> mirar las notas.
- Â¿Tiene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases mÃ©tricas. averiguar que otras mÃ©tricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser Ãºtil es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- AÃ±adir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de mediciÃ³n). El comportamiento lo detallarÃ© cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustarÃ­a hacer un script de bash que sea capaz de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo junto con algunas anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts estÃ¡n casi terminadas (faltan los mÃ©todos update). EsÃ³ sÃ­, la metricas de prompts  y la alimentaciÃ³n solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentacÃ­on de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es mÃ¡s dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan fÃ¡cil como otros lenguajes de programaciÃ³n).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo mÃ¡s fÃ¡cil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una mediciÃ³n-> parece que Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podrÃ­a afectar a la mediciÃ³n[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer mÃ©tricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metricas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issue anterior pueden ser Ãºtiles-> mirar y decidir luego.
    -  Ver si llama.cpp tambien las devuelve y si no como obtenerlas.-> si las devuelve.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todavÃ­a no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests mÃ¡s limpios en paralelo.
- SegÃºn el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, tengo demasiadas ventanas abiertas en el navegador ðŸ™ƒ)

# 2025-04-27: 
## InstalaciÃ³n y verificaciÃ³n del SO BASE
Hoy en esta tarde tan agradable de domingo voy a proceder a instalra el so en a otra tarjeta microsd (un sandisk extreme pro64 gb,sdxc 1, v30 A2 luego actualizarÃ© con maÅ›detalles ) a fin de tener un sistema operativo limpio donde realizar las mediciones: 
```bash
sudo apt update && sudo apt full-upgrade #actualiza el software a la Ãºltima versiÃ³n 
sudo rpi-eeprom-update # comprobamos el firmware
# (Mon 23 Sep 13:02:56 UTC 2024 (1727096576)
# no actualizo anque ahay una versiÃ³n disponble (spoiler actualizarÃ© luego)
```
Procedo a modicar el bootloader, activo el autologin(por defecto al ser la version lite de raspbian(la misma que se indica en so base), no tiene entorno de escritorio).
```bash 
sudo raspi-config
```
Procedo tambien a aumentar la swap a 1024 MB para por si acaso (previamente 512 ya que los modelos a ejecutar tienen versiones de parametros que rondan los 7b por lo que pueden necesitar mas ram de la disponible y por tanto hacer uso de la swap): 
```bash 
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile #CONF_SWAPSIZE=1024 
sudo dphys-swapfile setup 
sudo dphys-swapfile swapon # luego de instalar todo lo reiniciarÃ©
```
Procedo a comprobar el DVFS, aunque en la RPI5 el dvfs se controla con el gobernador, no sin antes mirar la documetaciÃ³n del [configtxt](https://www.raspberrypi.com/documentation/computers/config_txt.html) para ver que informaciÃ³n puede ser relevante luego -> no toco nada y lo dejo tal como estÃ¡ (tomo nota de que arm boost estÃ¡ a 1).
Compruebo el modo del gobernor(mientras buscaba info que referenciar para el gobernador, he encontrado esta pagina [web](https://bret.dk/raspberry-pi-5-review/) con varios benchmarks de la rpi, no tiene nada que ver pero quizas pueda ser Ãºtil en un futuro); el gobernador estÃ¡ puesto a ondemand, lo dejo asÃ­.
''' bash 
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor 
'''
Procedo a actualizar la eeprom .
```bash
sudo rpi-eeprom -update -a
```
Compruebo la versiÃ³n de python instalada-> python 3.11.2  y reinicio 

## InstalaciÃ³n del software de hailo()
Solo me voy a dedicar a hacer la instalaciÃ³n software(siguiendo la misma [guÃ­a](https://github.com/TFG-yisuscc/hailo-rpi5-examples/blob/main/doc/install-raspberry-pi5.md#how-to-set-up-raspberry-pi-5-and-hailo) que la vez anterior):

1.  habilito pcie3(recordatorio tambiÃ©n de porquÃ© estÃ¡ [deshabilitado por defecto](https://blog.adafruit.com/2023/10/13/forcing-pci-express-gen-3-0-speeds-on-the-pi-5-piday-raspberrypi-geerlingguy-raspberry_pi/))
2. desenchufo la rpi e instalo el AI KIT 
3. instalo las dependencias (parece que descarga paquetes de iconos y todo, el adwaita).
4. Reinicio y compruebo 
5. no lo detecta compruebo el cable que une a la rpi con el hat.
6. Funciona.
7. no desmonto el ai kit. 

Durante todo este proceso, he observado que los tiempos de instalaciÃ³n han disminuido bastante, al menos comparado con lo que recuerdo de la otra vez. -> muy problablemente sea por la nueva micro sd(no creo que sea por el firmware pero tambiÃ©n podrÃ­a deberse a eso). Es posible que tengas que abordarlo cuando elabores la memoria y muy probablemente tengas  que establecer una comparativa.

### Vuelvo a la tarjeta micro sd anterior(la verbatim):
1. Instalo los driver hailo,
2. Me aseguro de que el firmwre de la eeprom estÃ© actualizado
Verifico que a la hora de instalar los drivers la tarjeta verbatim es  mÃ¡s lenta.
Hago una micro  prueba de rendimiento. -> con el hailo instalado (y por ende sin el ventilador del la carcasa) rondaba los 100 grados segÃºn btop y empezÃ³ a a ralentizarse -> detuve la prueba por si acaso .

# 2025-05-01
(Los cambios los estoy estoy haciendolos en una (rama aparte[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_mejora_clase_prompt_metrics]))
La idea general sobre quÃ© mÃ©tricas considerar y cÃ³mo medirlas estÃ¡ mÃ¡s o menos definida (aunque con Ollama). 
El concepto es el siguiente: desarrollar un programa que, dado un conjunto de modelos, una frecuencia de muestreo y una lista de prompts, obtenga mÃ©tricas relacionadas con cada prompt y con el hardware. Esto se realizarÃ¡ de la siguiente forma , mientras se procesa cada prompt en paralelo, A intervalos regulares, se tomarÃ¡n medidas del hardware. Este proceso se repetirÃ¡ para cada modelo.

Falta lo siguiente(sin orden particular): 

- Mediciones relativas al sensor de temperatura de hailo [Enlace relevante](https://community.hailo.ai/t/how-to-get-temperature-of-hailo8l-device/1943).

- Pulir el cÃ³digo.

- Incorporar las funciones relacionadas con llama.cpp.

-Corregir pequeÃ±os errores en las clases Hardware metrics relacionados con los PIDS-> realmente no creo que sea un error 

- Separar las variables/parametros configurables por el usuario del programa(frecuencia,modelos espaciado temporal minimo entre prompts  o entre modelos...).

- Mirar si usar logs en lugar de csv.
 - Lo descrito en las issues del repositorio

Pero antes de  ponerme con eso, voy a hablar con Delia y Jorge antes de continuar, para asegurarme de que voy bien encaminado(que Ãºltimamente estoy despistadÃ­simo).
PD: Este [repo](https://github.com/aidatatools/ollama-benchmark.git) es interesante
# 2025-05-03

Viendo y comparando los tiempos en los prompt, me preocupa que la forma de medir actual estÃ© interfiriendo(no me extraÃ±arÃ­a en absoluto), ya que en comparaciÃ³n con la otra forma(la de dos hilos paralelos sin sincronizaciÃ³n) tarda mucho en llegar al 100% de cpu, mientras que cuando lo monitorizaba aparte con btop en cuanto se cargaba usaba el 100 de la cpu.-> correcciÃ³n puede deberse a que simplemente estÃ¡ cargando el modelo.

Con los exÃ¡menes  la vuelta de la esquina, voy a tener que dejarlo un poco aparcado, asi que me voy a establecer un roadmap :
1. Convertir las carpetas a paquetes(error de novato) y configurar correctamente el gitignore-> hecho, aunque probablemente tenga que mejorarlo luego
2. Completar llama.cpp.
3. Configurar el keepalive de ollama para que se desactive con el Ãºltmo prompt(en el caso de llama que se descarge de la memoria). 
2. Pasar de csv puro a log o mantenerlo 
3. Crear el configuration.py

# 2025-05-04: 
Bueno me dedico a continuar  las funciones relacionadas con llama.cpp, parece que aunque por defecto en los modelos LLama  devuelvan  las metrics de perf, estas no son tan fÃ¡cilmente "accesibles" ni estÃ¡n en la misma magnitud(milisegundos vs nanosegundos) como la de ollama, considero lo siguiente: 

1. Trastear con la forma en la que llama da las respuestas: 
    - Mediante cambios en el formato de chat que usa o la api que usa.
    - Mediante RegEx-> lo he intentado durante unas horas y lo voy a descartar, muy problemÃ¡tico y no queda limpio(para mi gusto)
2. Calcularlas manualmente, esto tiene el siguiente problema(entre otros): 
La marca de tiempo de fin - la de inicio no es coincidente con la duraciÃ³n total expresada por ollama, por ejemplo: 
```
prompt_id,start_timestamp,finish_timestamp,model,total_duration,prompt_eval_count,prompt_eval_duration,eval_count,load_duration
0,1746372420759336778,1746372530928341446,llama3.2:latest,110165606442,100,10625576984,343,34353355410
1,1746372531432656953,1746372598822507332,llama3.2:latest,67387207186,63,4713146930,326,34342030
```
 En la primera fila: 
 1746372530928341446-1746372420759336778 = 110169004668 != 110165606442 .
 Supone una diferencia de tiempo de aproximadaamente 3,39 milisegundos.

Mientras que en la segunda fila:
1746372598822507332-1746372531432656953 = 67389850379 != 67387207186
Lo que supone una diferencia de 2,64 milisegundos aproximadamente.
Aunque es poco, sin saber cuanta precisiÃ³n voy a necesitar, prefiero explorar esta opciÃ³n mas tarde si fuera necesario.

3. Viendo el (codigo fuente)[https://github.com/abetlen/llama-cpp-python/blob/99f2ebfde18912adeb7f714b49c1ddb624df3087/llama_cpp/llama_cpp.py#L4223] puedo crear una clase auxiliar que haga un parseo de los datos(quizÃ¡s  incluso una clase que la extienda)ya que la libreria de llama_cpp_python no lo ofrece  -> es lo que estoy haciendo ahora en la rama (dev_llama_cpp_function)[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_llama_cpp_functions] en utils.llama_utils, aunque tengo la duda de que llama al darte los datos en ms en lugar de ns (como hace ollama), Â¿a que tenemos que convertir?, ollama a ms o llama a ns , personalmente me decanto por lo primero, pero voy a esperar a maÃ±ana.

# 2025-05-08:
Rama llama.cpp functions
Me he dado cuenta de que faltaba el eval_duration en la clase promp_metrics-> corregido.


SegÃºn el codigo fuente de [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/51fb96b1ff2e1cc98b2492a012b7d93531a6a9a8/src/llama-context.cpp#L2428) el total duration se calcula restando el tiempo de t_start- t_end. Sin  embargo, como se puede observar, ese t_end no nos lo porporciona el objeto data,  sino que se obtine del propio sistema- > voy a aprovechar las timestamps.
# 2025-05-16
Antes de que se me olvide, este [entrada](https://learn.arm.com/learning-paths/embedded-and-microcontrollers/llama-python-cpu/llama-python-chatbot/) de la pagina de arm es interesante, sobretodo para las estimaciones de memoria ram

He estado estos dias limpiando el repositorio(mergeando ramas y eliminando ramas en remoto que ya no se usan, etc) y como bien dije en la presentaciÃ³n las funciones  de ollama  estÃ¡n completas.

Respecto a las funciones de llama, estÃ¡n casi listas y un poco mÃ¡s organizados(en una rama llamada llama_cpp_class_n_function, probalemente no la suba al remoto)
Falta por determinar como se realizarÃ¡n las mediciones de llama(es decir falta el equivalente de main_ollama pero para llama) y como se recibiran los modelos de llama(en ollama simplemente lo especificamos mediante una cadena de texto), supongo que lo harÃ© mediante rutas (absolutas) al archivo.

Me pongo a estudiar robÃ³tica y espero no morir en el intento.
# 2025-09-07

 Esto es lo que estoy (y he estado )haciendo en lo relativo  al software del proyecto: 
  1. Terminar y volcar todos los cambios de monitor system (la version de python) en main, no lo considero prioritario (y si no hay conflictos es trivial), ya estÃ¡ todo en dev y de desarrollarse  la versiÃ³n en cpp, probablemente no se use mÃ¡s .
  2. Reescribirlo en C++.
  3. Sigo intentando a ver si consigo compilar y ejecutar elib (por ahora no he recibido respuesta al correo que apareciÃ¡ en arxiv de Hao Chen, no sÃ© si procede escribirle al resto de autores). 

Relativo a los ultimos artÃ­culos que he recibido de Delia(generative egde ai de un tal paul y el del elib), he hecho lo mismo que el resto de papers(imprimirlos y e ir haciendo las anotaciones/ observaciones sobre el propio paper), debo mencionar que estos papers tienen a su vesz una bibliografia muy Ãºtil-> AÃ±adirla al zotero.
Sobre el zotero, tnego que organizarlo(Â¿quÃ© y cuÃ¡l estructura de directorios se adjusta mÃ¡s a mis necesidades?) e ir eliminado articulos que  no me sirven.junto con una unificaciÃ³n de las modificaciones que he estado realizando a los artÃ­culos en papel.
 ## 16:43
Volviendo al punto 3, no he conseguido avanzar apenas, estas son las cosas que he intentado y reintentado (las estoy enumerando de memoria): 

0. comprobar que no se me hubiera olvidado descargar algÃºn submÃ³dulo y descargado las carpetas de models y los. bin de los modelos
1. cambiar de posicion el cmakelist.txt
2.  comentar la de add executable(llama patatÃ­n pataÂ´an ) es decir las lineas 119 a 129
3. aÃ±adir  este codigo al final para generar los ejecutables del benchmark 
'''
foreach(bench_file ${BENCH_SRC})
  get_filename_component(bench_name ${bench_file} NAME_WE)
  add_executable(${bench_name} ${bench_file})
  target_link_libraries(${bench_name} ELIB)
  target_include_directories(${bench_name} PUBLIC include src)
endforeach()
'''
4. posteriormente he ido modificando los includes,segÃºn daban errores(generalmente los que hacian referencai a cabeceras ubicadas en las carpetar de kernel y de utils).

Considero que el proyecto  de ELIB en el repositorio estÃ¡ incompleto o dan por sentado algo que yo no estoy cayendo en la cuenta.

# 2025-09-018
Cntinuo con el porteo a cpp de monitor system, el problema que mas quebraderos de cabeza me estÃ¡ dando es el CMAKEList.txt y el CLION, que no se porque razon no me reconoce bien los ficheros y me estÃ¡ dando errores de compilaciÃ³n y linkado de ficheros .h de otros  subdirectorios (todavÃ­a no he dado con la causa del fallo, pero al menos sÃ© cual es el fallo).
 
Las partes que llevo implementadas, por ahora funcionan bien de forma individual (memoria ) sin embargo hasta que no solucione los errores de compilacion y de linkado del proyecto, poco puedo hacer(y mira que llevo ias buscando como pero lo intentos y las chapucillas que hago estÃ¡n resultando infructuaosas.)

estoy por crear un nuevo repositorio y proyecto y ver si asÃ­ va. Pero curiosamente si funciona usando cmake desde la linea de comandos.

# 2025-09-22

 He estado liado con algunas cosillas(IOTUS), asÃ­ que no le he dado tanta caÃ±a. Actualizo lo de estos Ãºltimos dÃ­as, 
 He conseguido configurar el proyecto y el repositorio para el porteo a C++, quee no sÃ© por quÃ© razÃ³n ha dado tantos problemas, y configurado el gestor de dependencias(que tambien ha dado sus problemas...). En definitiva todo lo que podÃ­a haber salido mal, ha ido mal.
  
He corregido algunos de los errores que se presentaron al fusionar la rama dev con main del proyecto monitorSystem(python version).
En la versÃ­on de python, quedarÃ­a por aÃ±adir las siguientes caracteristicas: 
- UnificacÃ­on de los ficheros de metriicas-> trabajar con un solo fichero.
- Guardar el prompt y la respuesta -> para esto es mejor jsonlines 
En el porteo a cpp: 
- Desarrollado la clase de memoria 
- Empezado el logger.
