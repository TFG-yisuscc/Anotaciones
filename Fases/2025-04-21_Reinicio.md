# Razones 
me estoy llendo por los cerros de 칰beda. En un momneto dado estoy revisando (o a침adiendo)bibliograf칤a, que viendo como funciona stui, pasando por un c칩mo ser칤a reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementac칤on en python,por esta raz칩n voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto
1. Siguiendo los pasos en el fihero installaci칩nSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desrrollar de vforma m치s cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la m치quina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a trav칠s de ssh.
3. Intento de instalaci칩n del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalaci칩n de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios d칤as)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librer칤a ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(쯗e forma binaria o discreta?)-> mirar uno de los papers de delia y jorge, se menciona el ventilador .
- Buscar una forma m치s fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparaci칩n btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se est치 haciendo la medici칩n baja la utilizaci칩n de la cpu al no atenderse el promt(쯔l ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reuni칩n mencion칩 algo de la configuraci칩n throtlling- 쯈u칠 era exactamente? -> mirar las notas.
- 쯊iene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases m칠tricas. averiguar que otras m칠tricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser 칰til es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- A침adir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de medici칩n). El comportamiento lo detallar칠 cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustar칤a hacer un script de bash que sea capad de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo pay algnusa anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts est치n casi terminadas (faltan los m칠todos update). Es칩 s칤, la metricas de prompts  y la alimentaci칩n solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentac칤on de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es m치s dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan f치cil como otros lenguajes de programaci칩n).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo m치s f치cil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una medici칩n-> Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podr칤a afectar a la medici칩n[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer m칠tricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metrcas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issiue anterior pueden ser 칰tiles.
    - [] Ver si llama.cpp tambien las devuelve y si no como obtenerlas.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todav칤a no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests m치s limpios en paralelo.
- Seg칰n el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, , tengo demasiadas ventanas abiertas en el navegador 游뗶)




