# Razones 
me estoy llendo por los cerros de úbeda. En un momneto dado estoy revisando (o añadiendo)bibliografía, que viendo como funciona stui, pasando por un cómo sería reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementacíon en python,por esta razón voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto
1. Siguiendo los pasos en el fihero installaciónSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desrrollar de vforma más cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la máquina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a través de ssh.
3. Intento de instalación del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalación de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios días)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librería ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(¿de forma binaria o discreta?)-> mirar uno de los papers de delia y jorge, se menciona el ventilador .
- Buscar una forma más fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparación btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se está haciendo la medición baja la utilización de la cpu al no atenderse el promt(¿al ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reunión mencionó algo de la configuración throtlling- ¿Qué era exactamente? -> mirar las notas.
- ¿Tiene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases métricas. averiguar que otras métricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser útil es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- Añadir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de medición). El comportamiento lo detallaré cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustaría hacer un script de bash que sea capad de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo pay algnusa anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts están casi terminadas (faltan los métodos update). Esó sí, la metricas de prompts  y la alimentación solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentacíon de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es más dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan fácil como otros lenguajes de programación).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo más fácil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una medición-> Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podría afectar a la medición[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer métricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metrcas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issiue anterior pueden ser útiles.
    - [] Ver si llama.cpp tambien las devuelve y si no como obtenerlas.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todavía no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests más limpios en paralelo.
- Según el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, , tengo demasiadas ventanas abiertas en el navegador 🙃)




