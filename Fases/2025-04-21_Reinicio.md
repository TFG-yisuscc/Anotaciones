# Razones 
me estoy yendo por los cerros de 칰beda. En un momneto dado estoy revisando (o a침adiendo)bibliograf칤a, que viendo como funciona stui, pasando por un c칩mo ser칤a reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementac칤on en python,por esta raz칩n voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto  
1. Siguiendo los pasos en el fichero instalaci칩nSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desarrollar de forma m치s cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la m치quina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a trav칠s de ssh.
3. Intento de instalaci칩n del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalaci칩n de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios d칤as)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librer칤a ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(쯗e forma binaria o discreta?)-> mirar uno de los papers de Delia y Jorge, se menciona el ventilador .
- Buscar una forma m치s fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparaci칩n btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se est치 haciendo la medici칩n baja la utilizaci칩n de la cpu al no atenderse el promt(쯔l ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reuni칩n mencion칩 algo de la configuraci칩n throtlling- 쯈u칠 era exactamente? -> mirar las notas.
- 쯊iene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases m칠tricas. averiguar que otras m칠tricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser 칰til es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- A침adir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de medici칩n). El comportamiento lo detallar칠 cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustar칤a hacer un script de bash que sea capaz de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo junto con algunas anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts est치n casi terminadas (faltan los m칠todos update). Es칩 s칤, la metricas de prompts  y la alimentaci칩n solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentac칤on de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es m치s dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan f치cil como otros lenguajes de programaci칩n).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo m치s f치cil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una medici칩n-> parece que Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podr칤a afectar a la medici칩n[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer m칠tricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metricas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issue anterior pueden ser 칰tiles-> mirar y decidir luego.
    -  Ver si llama.cpp tambien las devuelve y si no como obtenerlas.-> si las devuelve.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todav칤a no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests m치s limpios en paralelo.
- Seg칰n el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, tengo demasiadas ventanas abiertas en el navegador 游뗶)

# 2025-04-27: 
## Instalaci칩n y verificaci칩n del SO BASE
Hoy en esta tarde tan agradable de domingo voy a proceder a instalra el so en a otra tarjeta microsd (un sandisk extreme pro64 gb,sdxc 1, v30 A2 luego actualizar칠 con ma콑detalles ) a fin de tener un sistema operativo limpio donde realizar las mediciones: 
```bash
sudo apt update && sudo apt full-upgrade #actualiza el software a la 칰ltima versi칩n 
sudo rpi-eeprom-update # comprobamos el firmware
# (Mon 23 Sep 13:02:56 UTC 2024 (1727096576)
# no actualizo anque ahay una versi칩n disponble (spoiler actualizar칠 luego)
```
Procedo a modicar el bootloader, activo el autologin(por defecto al ser la version lite de raspbian(la misma que se indica en so base), no tiene entorno de escritorio).
```bash 
sudo raspi-config
```
Procedo tambien a aumentar la swap a 1024 MB para por si acaso (previamente 512 ya que los modelos a ejecutar tienen versiones de parametros que rondan los 7b por lo que pueden necesitar mas ram de la disponible y por tanto hacer uso de la swap): 
```bash 
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile #CONF_SWAPSIZE=1024 
sudo dphys-swapfile setup 
sudo dphys-swapfile swapon # luego de instalar todo lo reiniciar칠
```
Procedo a comprobar el DVFS, aunque en la RPI5 el dvfs se controla con el gobernador, no sin antes mirar la documetaci칩n del [configtxt](https://www.raspberrypi.com/documentation/computers/config_txt.html) para ver que informaci칩n puede ser relevante luego -> no toco nada y lo dejo tal como est치 (tomo nota de que arm boost est치 a 1).
Compruebo el modo del gobernor(mientras buscaba info que referenciar para el gobernador, he encontrado esta pagina [web](https://bret.dk/raspberry-pi-5-review/) con varios benchmarks de la rpi, no tiene nada que ver pero quizas pueda ser 칰til en un futuro); el gobernador est치 puesto a ondemand, lo dejo as칤.
''' bash 
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor 
'''
Procedo a actualizar la eeprom .
```bash
sudo rpi-eeprom -update -a
```
Compruebo la versi칩n de python instalada-> python 3.11.2  y reinicio 

## Instalaci칩n del software de hailo()
Solo me voy a dedicar a hacer la instalaci칩n software(siguiendo la misma [gu칤a](https://github.com/TFG-yisuscc/hailo-rpi5-examples/blob/main/doc/install-raspberry-pi5.md#how-to-set-up-raspberry-pi-5-and-hailo) que la vez anterior):

1.  habilito pcie3(recordatorio tambi칠n de porqu칠 est치 [deshabilitado por defecto](https://blog.adafruit.com/2023/10/13/forcing-pci-express-gen-3-0-speeds-on-the-pi-5-piday-raspberrypi-geerlingguy-raspberry_pi/))
2. desenchufo la rpi e instalo el AI KIT 
3. instalo las dependencias (parece que descarga paquetes de iconos y todo, el adwaita).
4. Reinicio y compruebo 
5. no lo detecta compruebo el cable que une a la rpi con el hat.
6. Funciona.
7. no desmonto el ai kit. 

Durante todo este proceso, he observado que los tiempos de instalaci칩n han disminuido bastante, al menos comparado con lo que recuerdo de la otra vez. -> muy problablemente sea por la nueva micro sd(no creo que sea por el firmware pero tambi칠n podr칤a deberse a eso). Es posible que tengas que abordarlo cuando elabores la memoria y muy probablemente tengas  que establecer una comparativa.

### Vuelvo a la tarjeta micro sd anterior(la verbatim):
1. Instalo los driver hailo,
2. Me aseguro de que el firmwre de la eeprom est칠 actualizado
Verifico que a la hora de instalar los drivers la tarjeta verbatim es  m치s lenta.
Hago una micro  prueba de rendimiento. -> con el hailo instalado (y por ende sin el ventilador del la carcasa) rondaba los 100 grados seg칰n btop y empez칩 a a ralentizarse -> detuve la prueba por si acaso .

# 2025-05-01
(Los cambios los estoy estoy haciendolos en una (rama aparte[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_mejora_clase_prompt_metrics]))
La idea general sobre qu칠 m칠tricas considerar y c칩mo medirlas est치 m치s o menos definida (aunque con Ollama). 
El concepto es el siguiente: desarrollar un programa que, dado un conjunto de modelos, una frecuencia de muestreo y una lista de prompts, obtenga m칠tricas relacionadas con cada prompt y con el hardware. Esto se realizar치 de la siguiente forma , mientras se procesa cada prompt en paralelo, A intervalos regulares, se tomar치n medidas del hardware. Este proceso se repetir치 para cada modelo.

Falta lo siguiente(sin orden particular): 

- Mediciones relativas al sensor de temperatura de hailo [Enlace relevante](https://community.hailo.ai/t/how-to-get-temperature-of-hailo8l-device/1943).

- Pulir el c칩digo.

- Incorporar las funciones relacionadas con llama.cpp.

-Corregir peque침os errores en las clases Hardware metrics relacionados con los PIDS-> realmente no creo que sea un error 

- Separar las variables/parametros configurables por el usuario del programa(frecuencia,modelos espaciado temporal minimo entre prompts  o entre modelos...).

- Mirar si usar logs en lugar de csv.
 - Lo descrito en las issues del repositorio

Pero antes de  ponerme con eso, voy a hablar con Delia y Jorge antes de continuar, para asegurarme de que voy bien encaminado(que 칰ltimamente estoy despistad칤simo).
PD: Este [repo](https://github.com/aidatatools/ollama-benchmark.git) es interesante
# 2025-05-03

Viendo y comparando los tiempos en los prompt, me preocupa que la forma de medir actual est칠 interfiriendo(no me extra침ar칤a en absoluto), ya que en comparaci칩n con la otra forma(la de dos hilos paralelos sin sincronizaci칩n) tarda mucho en llegar al 100% de cpu, mientras que cuando lo monitorizaba aparte con btop en cuanto se cargaba usaba el 100 de la cpu.-> correcci칩n puede deberse a que simplemente est치 cargando el modelo.

Con los ex치menes  la vuelta de la esquina, voy a tener que dejarlo un poco aparcado, asi que me voy a establecer un roadmap :
1. Convertir las carpetas a paquetes(error de novato) y configurar correctamente el gitignore-> hecho, aunque probablemente tenga que mejorarlo luego
2. Completar llama.cpp.
3. Configurar el keepalive de ollama para que se desactive con el 칰ltmo prompt(en el caso de llama que se descarge de la memoria). 
2. Pasar de csv puro a log o mantenerlo 
3. Crear el configuration.py

# 2025-05-04: 
Bueno me dedico a continuar  las funciones relacionadas con llama.cpp, parece que aunque por defecto en los modelos LLama  devuelvan  las metrics de perf, estas no son tan f치cilmente "accesibles" ni est치n en la misma magnitud(milisegundos vs nanosegundos) como la de ollama, considero lo siguiente: 

1. Trastear con la forma en la que llama da las respuestas: 
    - Mediante cambios en el formato de chat que usa o la api que usa.
    - Mediante RegEx-> lo he intentado durante unas horas y lo voy a descartar, muy problem치tico y no queda limpio(para mi gusto)
2. Calcularlas manualmente, esto tiene el siguiente problema(entre otros): 
La marca de tiempo de fin - la de inicio no es coincidente con la duraci칩n total expresada por ollama, por ejemplo: 
```
prompt_id,start_timestamp,finish_timestamp,model,total_duration,prompt_eval_count,prompt_eval_duration,eval_count,load_duration
0,1746372420759336778,1746372530928341446,llama3.2:latest,110165606442,100,10625576984,343,34353355410
1,1746372531432656953,1746372598822507332,llama3.2:latest,67387207186,63,4713146930,326,34342030
```
 En la primera fila: 
 1746372530928341446-1746372420759336778 = 110169004668 != 110165606442 .
 Supone una diferencia de tiempo de aproximadaamente 3,39 milisegundos.

Mientras que en la segunda fila:
1746372598822507332-1746372531432656953 = 67389850379 != 67387207186
Lo que supone una diferencia de 2,64 milisegundos aproximadamente.
Aunque es poco, sin saber cuanta precisi칩n voy a necesitar, prefiero explorar esta opci칩n mas tarde si fuera necesario.

3. Viendo el (codigo fuente)[https://github.com/abetlen/llama-cpp-python/blob/99f2ebfde18912adeb7f714b49c1ddb624df3087/llama_cpp/llama_cpp.py#L4223] puedo crear una clase auxiliar que haga un parseo de los datos(quiz치s  incluso una clase que la extienda)ya que la libreria de llama_cpp_python no lo ofrece  -> es lo que estoy haciendo ahora en la rama (dev_llama_cpp_function)[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_llama_cpp_functions] en utils.llama_utils, aunque tengo la duda de que llama al darte los datos en ms en lugar de ns (como hace ollama), 쯔 que tenemos que convertir?, ollama a ms o llama a ns , personalmente me decanto por lo primero, pero voy a esperar a ma침ana.

# 2025-05-08:
Rama llama.cpp functions
Me he dado cuenta de que faltaba el eval_duration en la clase promp_metrics-> corregido.


Seg칰n el codigo fuente de [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/51fb96b1ff2e1cc98b2492a012b7d93531a6a9a8/src/llama-context.cpp#L2428) el total duration se calcula restando el tiempo de t_start- t_end. Sin  embargo, como se puede observar, ese t_end no nos lo porporciona el objeto data,  sino que se obtine del propio sistema- > voy a aprovechar las timestamps.
# 2025-05-16
Antes de que se me olvide, este [entrada](https://learn.arm.com/learning-paths/embedded-and-microcontrollers/llama-python-cpu/llama-python-chatbot/) de la pagina de arm es interesante, sobretodo para las estimaciones de memoria ram

He estado estos dias limpiando el repositorio(mergeando ramas y eliminando ramas en remoto que ya no se usan, etc) y como bien dije en la presentaci칩n las funciones  de ollama  est치n completas.

Respecto a las funciones de llama, est치n casi listas y un poco m치s organizados(en una rama llamada llama_cpp_class_n_function, probalemente no la suba al remoto)
Falta por determinar como se realizar치n las mediciones de llama(es decir falta el equivalente de main_ollama pero para llama) y como se recibiran los modelos de llama(en ollama simplemente lo especificamos mediante una cadena de texto), supongo que lo har칠 mediante rutas (absolutas) al archivo.

Me pongo a estudiar rob칩tica y espero no morir en el intento.
# 2025-09-07

 Esto es lo que estoy (y he estado )haciendo en lo relativo  al software del proyecto: 
  1. Terminar y volcar todos los cambios de monitor system (la version de python) en main, no lo considero prioritario (y si no hay conflictos es trivial), ya est치 todo en dev y de desarrollarse  la vers칤on en cpp, probablemente no se use m치s .
  2. Reescribirlo en C++.
  3. Sigo intentando a ver si consigo compilar y ejecutar elib (por ahora no he recibido respuesta al correo que apareci치 en arxiv de Hao Chen, no s칠 si procede escribirle al resto de autores). 

Relativo a los ultimos art칤culos que he recibido de Delia(generative egde ai de un tal paul y el del elib), he hecho lo mismo que el resto de papers(imprimirlos y e ir haciendo las anotaciones/ observaciones sobre el propio paper), debo mencionar que estos papers tienen a su vesz una bibliografia muy 칰til-> A침adirla al zotero.
Sobre el zotero, tnego que organizarlo(쯤ue y cual estructura de directorios se adjusta m치s a mis necesidades?) e ir eliminado articulos que o bien no he leido o bien no me sirven.junto con una unificaci칩n de las modificaciones que he estado realizando a los art칤culos en papel.
