# Razones 
me estoy llendo por los cerros de Ãºbeda. En un momneto dado estoy revisando (o aÃ±adiendo)bibliografÃ­a, que viendo como funciona stui, pasando por un cÃ³mo serÃ­a reescribir el programa en rust, sin siquiera  haber conseguido hacer una implementacÃ­on en python,por esta razÃ³n voy reiniciar el proyecto.
# Lo que llevo hecho desde el reinicio hasta ahora
## Configurar la RPI para el desarrollo remoto
1. Siguiendo los pasos en el fichero instalaciÃ³nSO base y  omitiendo los pasos dedicados al hailo ai kit , ya que solo pretendo tener un sitio donde desarrollar de forma mÃ¡s cercana al hardware (los comandos vgencmd no funcionan en mi portatil) he instalado y configurando el SO, salvo por el hecho de que la mÃ¡quina se llama ahora raspberrypi2.local. 
2. he instalado y configurado Git para usarlo a travÃ©s de ssh.
3. Intento de instalaciÃ³n del pycharm remoto.
4. aumento de la swap a 1GB.
5. Reintento la instalaciÃ³n de pycharm (estos 3 ultimos pasos, varias veces a lo largo de varios dÃ­as)
6. desisto e instalo el remoto de vscode.
7. procedo a corregir y depurar tods los errores previos (hasta entonces habia tenido que programar a ciegas).
8. Instalo ollama(solo el script y para probar uno de los modelos de gemma- parece que funciona bastante bien con la cpu), no lo expongo como servidor remoto.
9. intento instalar la librerÃ­a ollama de python-> fallido-> pruebo a crear un [entorno virtual](https://python.land/virtual-environments/virtualenv) he instalado las librerias que creo que me van a hacer falta(ollama, huggingface y llama en el requirements.txt)-> funciona.
10. Pruebo lo programado y parece que funciona.
## Que pretendo hacer a partir de ahora: 
- Buscar la forma de medir la velocidad del ventilador(Â¿de forma binaria o discreta?)-> mirar uno de los papers de Delia y Jorge, se menciona el ventilador .
- Buscar una forma mÃ¡s fiable de medir la carga de la cpu, Actualmente reporta unos valores de IDLE muy altos para la tarea(en comparaciÃ³n btop reporta en torno un 97% de uso de cpu). Conjeturo que puede deberse a que justo cuando se estÃ¡ haciendo la mediciÃ³n baja la utilizaciÃ³n de la cpu al no atenderse el promt(Â¿al ser multicore no deberia poderse ejecutar en paralelo directamente? -> ver apuntes ac y las referencias de threading y del get_mem).
- Delia en la reuniÃ³n mencionÃ³ algo de la configuraciÃ³n throtlling- Â¿QuÃ© era exactamente? -> mirar las notas.
- Â¿Tiene la placa hailo sensores?. En caso afirmativo buscar como acceder a ellos.
- Homogeneizar las clases mÃ©tricas. averiguar que otras mÃ©tricas hardware son necesarias.
- Que produzca ficheros con los resutados, el  primero contiene las mediciones y el otro parametros relacionados con el promt(el tiempo de inferenciaen 2 timestamps, el promt de entrada y el respuesta de salida). Algo que creo que puede ser Ãºtil es poderevaluar la calidad de las respuestasa posteriori ya que los promts que uso son para evaluar respuestas. 
- AÃ±adir unos selectores (modelo y origen(ollama o llama),y lista de promts, frecuencia de mediciÃ³n). El comportamiento lo detallarÃ© cuando vaya a implementarlo.
- Fuera ya del monitoreo del sistema, me gustarÃ­a hacer un script de bash que sea capaz de configurar e instalar una rpi "limpia" con todos los programas necesarios.

# 2025-04-24/25/26:
Como viene siendo costumbre escribo lo que he estado haciendo junto con algunas anotaciones y tareas para mi futuro yo
-  La estructuras de las metricas de hardware y de prompts estÃ¡n casi terminadas (faltan los mÃ©todos update). EsÃ³ sÃ­, la metricas de prompts  y la alimentaciÃ³n solo funciona con ollama
    - [] Buscar la forma de integrar llama y obtener ambas mediciones.
- Respecto a las mediciones hardware, hay que determinar como se van a hacer, 3 formas:
    -  Completamente paralelo a la alimentacÃ­on de prompts(un hilo alimenta y otro toma medidas con cierta frecuencia, es el que estoy usando ahora para las pruebas, etiquetar las mediciones hardware es mÃ¡s dificil(pero no tanto), requiere variables compartidas y python eso no le gusta ponerlo tan fÃ¡cil como otros lenguajes de programaciÃ³n).
    -  Semiparalelo: por cada propmt se inicia en paralelo un bucle de mediciones que relaiza las mediciones con cierta frecuencia(etiquetarlo se vuelve algo mÃ¡s fÃ¡cil).
    - A posteriori, Justo cuando se termina un prompt, se realiza una mediciÃ³n-> parece que Ollama hace optimizaciones de carga y descarga de los modelos,dependiendo de como lo implemente , podrÃ­a afectar a la mediciÃ³n[Enlace](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025).
- Existen programas/ complementos para ollama que ayudan a establecer mÃ©tricas como [Prometheus](https://www.belsterns.com/post/ollama-vs-llama-cpp-which-one-should-you-choose-in-2025),  u [openLit](https://github.com/openlit/openlit?tab=readme-ov-file).y en el github parece que hay [demanda](https://github.com/ollama/ollama/issues/3144). Sin embargo, para la clase prompt metrics he utilizado las propias metricas que devuelve ollama con la respuesta[enlace](https://github.com/ollama/ollama/blob/main/docs/api.md). Aunque las mediciones que se proponen en la issue anterior pueden ser Ãºtiles-> mirar y decidir luego.
    -  Ver si llama.cpp tambien las devuelve y si no como obtenerlas.-> si las devuelve.
- En las mediciones hardware, hay un periodo inicial donde parece que el modelo todavÃ­a no se ha arrancado(lo supongo por las bajas temperaturas y el uso de la cpu ).-> Determinar si se ha arrancado y alimentado y si no solucionar.
- Voy a buscar otra tarjeta micro sd para tener una limpia y poder hacer tests mÃ¡s limpios en paralelo.
- SegÃºn el [foro oficial](https://community.hailo.ai/t/measuring-power-consumption-on-hailo8l-with-rp5-ai-kit/5234) al hailo no tiene sensores de consumo de potencia, pero creo que se menciona un sensor de temperatura (no se si era en el enlace adjunto o en otro, tengo demasiadas ventanas abiertas en el navegador ðŸ™ƒ)

# 2025-04-27: 
## InstalaciÃ³n y verificaciÃ³n del SO BASE
Hoy en esta tarde tan agradable de domingo voy a proceder a instalra el so en a otra tarjeta microsd (un sandisk extreme pro64 gb,sdxc 1, v30 A2 luego actualizarÃ© con maÅ›detalles ) a fin de tener un sistema operativo limpio donde realizar las mediciones: 
```bash
sudo apt update && sudo apt full-upgrade #actualiza el software a la Ãºltima versiÃ³n 
sudo rpi-eeprom-update # comprobamos el firmware
# (Mon 23 Sep 13:02:56 UTC 2024 (1727096576)
# no actualizo anque ahay una versiÃ³n disponble (spoiler actualizarÃ© luego)
```
Procedo a modicar el bootloader, activo el autologin(por defecto al ser la version lite de raspbian(la misma que se indica en so base), no tiene entorno de escritorio).
```bash 
sudo raspi-config
```
Procedo tambien a aumentar la swap a 1024 MB para por si acaso (previamente 512 ya que los modelos a ejecutar tienen versiones de parametros que rondan los 7b por lo que pueden necesitar mas ram de la disponible y por tanto hacer uso de la swap): 
```bash 
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile #CONF_SWAPSIZE=1024 
sudo dphys-swapfile setup 
sudo dphys-swapfile swapon # luego de instalar todo lo reiniciarÃ©
```
Procedo a comprobar el DVFS, aunque en la RPI5 el dvfs se controla con el gobernador, no sin antes mirar la documetaciÃ³n del [configtxt](https://www.raspberrypi.com/documentation/computers/config_txt.html) para ver que informaciÃ³n puede ser relevante luego -> no toco nada y lo dejo tal como estÃ¡ (tomo nota de que arm boost estÃ¡ a 1).
Compruebo el modo del gobernor(mientras buscaba info que referenciar para el gobernador, he encontrado esta pagina [web](https://bret.dk/raspberry-pi-5-review/) con varios benchmarks de la rpi, no tiene nada que ver pero quizas pueda ser Ãºtil en un futuro); el gobernador estÃ¡ puesto a ondemand, lo dejo asÃ­.
''' bash 
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor 
'''
Procedo a actualizar la eeprom .
```bash
sudo rpi-eeprom -update -a
```
Compruebo la versiÃ³n de python instalada-> python 3.11.2  y reinicio 

## InstalaciÃ³n del software de hailo()
Solo me voy a dedicar a hacer la instalaciÃ³n software(siguiendo la misma [guÃ­a](https://github.com/TFG-yisuscc/hailo-rpi5-examples/blob/main/doc/install-raspberry-pi5.md#how-to-set-up-raspberry-pi-5-and-hailo) que la vez anterior):

1.  habilito pcie3(recordatorio tambiÃ©n de porquÃ© estÃ¡ [deshabilitado por defecto](https://blog.adafruit.com/2023/10/13/forcing-pci-express-gen-3-0-speeds-on-the-pi-5-piday-raspberrypi-geerlingguy-raspberry_pi/))
2. desenchufo la rpi e instalo el AI KIT 
3. instalo las dependencias (parece que descarga paquetes de iconos y todo, el adwaita).
4. Reinicio y compruebo 
5. no lo detecta compruebo el cable que une a la rpi con el hat.
6. Funciona.
7. no desmonto el ai kit. 

Durante todo este proceso, he observado que los tiempos de instalaciÃ³n han disminuido bastante, al menos comparado con lo que recuerdo de la otra vez. -> muy problablemente sea por la nueva micro sd(no creo que sea por el firmware pero tambiÃ©n podrÃ­a deberse a eso). Es posible que tengas que abordarlo cuando elabores la memoria y muy probablemente tengas  que establecer una comparativa.

### Vuelvo a la tarjeta micro sd anterior(la verbatim):
1. Instalo los driver hailo,
2. Me aseguro de que el firmwre de la eeprom estÃ© actualizado
Verifico que a la hora de instalar los drivers la tarjeta verbatim es  mÃ¡s lenta.
Hago una micro  prueba de rendimiento. -> con el hailo instalado (y por ende sin el ventilador del la carcasa) rondaba los 100 grados segÃºn btop y empezÃ³ a a ralentizarse -> detuve la prueba por si acaso .

# 2025-05-01
(Los cambios los estoy estoy haciendolos en una (rama aparte[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_mejora_clase_prompt_metrics]))
La idea general sobre quÃ© mÃ©tricas considerar y cÃ³mo medirlas estÃ¡ mÃ¡s o menos definida (aunque con Ollama). 
El concepto es el siguiente: desarrollar un programa que, dado un conjunto de modelos, una frecuencia de muestreo y una lista de prompts, obtenga mÃ©tricas relacionadas con cada prompt y con el hardware. Esto se realizarÃ¡ de la siguiente forma , mientras se procesa cada prompt en paralelo, A intervalos regulares, se tomarÃ¡n medidas del hardware. Este proceso se repetirÃ¡ para cada modelo.

Falta lo siguiente(sin orden particular): 

- Mediciones relativas al sensor de temperatura de hailo [Enlace relevante](https://community.hailo.ai/t/how-to-get-temperature-of-hailo8l-device/1943).

- Pulir el cÃ³digo.

- Incorporar las funciones relacionadas con llama.cpp.

-Corregir pequeÃ±os errores en las clases Hardware metrics relacionados con los PIDS-> realmente no creo que sea un error 

- Separar las variables/parametros configurables por el usuario del programa(frecuencia,modelos espaciado temporal minimo entre prompts  o entre modelos...).

- Mirar si usar logs en lugar de csv.
 - Lo descrito en las issues del repositorio

Pero antes de  ponerme con eso, voy a hablar con Delia y Jorge antes de continuar, para asegurarme de que voy bien encaminado(que Ãºltimamente estoy despistadÃ­simo).
PD: Este [repo](https://github.com/aidatatools/ollama-benchmark.git) es interesante
# 2025-05-03

Viendo y comparando los tiempos en los prompt, me preocupa que la forma de medir actual estÃ© interfiriendo(no me extraÃ±arÃ­a en absoluto), ya que en comparaciÃ³n con la otra forma(la de dos hilos paralelos sin sincronizaciÃ³n) tarda mucho en llegar al 100% de cpu, mientras que cuando lo monitorizaba aparte con btop en cuanto se cargaba usaba el 100 de la cpu.-> correcciÃ³n puede deberse a que simplemente estÃ¡ cargando el modelo.

Con los exÃ¡menes  la vuelta de la esquina, voy a tener que dejarlo un poco aparcado, asi que me voy a establecer un roadmap :
1. Convertir las carpetas a paquetes(error de novato) y configurar correctamente el gitignore-> hecho, aunque probablemente tenga que mejorarlo luego
2. Completar llama.cpp.
3. Configurar el keepalive de ollama para que se desactive con el Ãºltmo prompt(en el caso de llama que se descarge de la memoria). 
2. Pasar de csv puro a log o mantenerlo 
3. Crear el configuration.py

# 2025-05-04: 
Bueno me dedico a continuar  las funciones relacionadas con llama.cpp, parece que aunque por defecto en los modelos LLama  devuelvan  las metrics de perf, estas no son tan fÃ¡cilmente "accesibles" ni estÃ¡n en la misma magnitud(milisegundos vs nanosegundos) como la de ollama, considero lo siguiente: 

1. Trastear con la forma en la que llama da las respuestas: 
    - Mediante cambios en el formato de chat que usa o la api que usa.
    - Mediante RegEx-> lo he intentado durante unas horas y lo voy a descartar, muy problemÃ¡tico y no queda limpio(para mi gusto)
2. Calcularlas manualmente, esto tiene el siguiente problema(entre otros): 
La marca de tiempo de fin - la de inicio no es coincidente con la duraciÃ³n total expresada por ollama, por ejemplo: 
```
prompt_id,start_timestamp,finish_timestamp,model,total_duration,prompt_eval_count,prompt_eval_duration,eval_count,load_duration
0,1746372420759336778,1746372530928341446,llama3.2:latest,110165606442,100,10625576984,343,34353355410
1,1746372531432656953,1746372598822507332,llama3.2:latest,67387207186,63,4713146930,326,34342030
```
 En la primera fila: 
 1746372530928341446-1746372420759336778 = 110169004668 != 110165606442 .
 Supone una diferencia de tiempo de aproximadaamente 3,39 milisegundos.

Mientras que en la segunda fila:
1746372598822507332-1746372531432656953 = 67389850379 != 67387207186
Lo que supone una diferencia de 2,64 milisegundos aproximadamente.
Aunque es poco, sin saber cuanta precisiÃ³n voy a necesitar, prefiero explorar esta opciÃ³n mas tarde si fuera necesario.

3. Viendo el (codigo fuente)[https://github.com/abetlen/llama-cpp-python/blob/99f2ebfde18912adeb7f714b49c1ddb624df3087/llama_cpp/llama_cpp.py#L4223] puedo crear una clase auxiliar que haga un parseo de los datos(quizÃ¡s  incluso una clase que la extienda)ya que la libreria de llama_cpp_python no lo ofrece  -> es lo que estoy haciendo ahora en la rama (dev_llama_cpp_function)[https://github.com/TFG-yisuscc/MonitorSystem/tree/dev_llama_cpp_functions] en utils.llama_utils, aunque tengo la duda de que llama al darte los datos en ms en lugar de ns (como hace ollama), Â¿a que tenemos que convertir?, ollama a ms o llama a ns , personalmente me decanto por lo primero, pero voy a esperar a maÃ±ana.


